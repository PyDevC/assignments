{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Disaster Response Pipeline App\n",
    "\n",
    "## ETL Pipeline Preparation For Disaster Messages Classification\n",
    "Follow the instructions below to help you create your ETL pipeline.\n",
    "### 1. Import libraries and load datasets.\n",
    "- Import Python libraries\n",
    "- Load `messages.csv` into a dataframe and inspect the first few lines.\n",
    "- Load `categories.csv` into a dataframe and inspect the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:10.290985Z",
     "iopub.status.busy": "2021-09-24T18:43:10.290402Z",
     "iopub.status.idle": "2021-09-24T18:43:12.359571Z",
     "shell.execute_reply": "2021-09-24T18:43:12.358687Z",
     "shell.execute_reply.started": "2021-09-24T18:43:10.290847Z"
    }
   },
   "outputs": [],
   "source": [
    "# import necessary libraries \n",
    "from sqlalchemy import create_engine\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet'])\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:12.361955Z",
     "iopub.status.busy": "2021-09-24T18:43:12.361627Z",
     "iopub.status.idle": "2021-09-24T18:43:12.628274Z",
     "shell.execute_reply": "2021-09-24T18:43:12.627391Z",
     "shell.execute_reply.started": "2021-09-24T18:43:12.361914Z"
    }
   },
   "outputs": [],
   "source": [
    "# load messages dataset\n",
    "msg_df = pd.read_csv(\"../input/disaster-response-messages/disaster_messages.csv\")\n",
    "msg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:12.63026Z",
     "iopub.status.busy": "2021-09-24T18:43:12.629916Z",
     "iopub.status.idle": "2021-09-24T18:43:12.951266Z",
     "shell.execute_reply": "2021-09-24T18:43:12.950617Z",
     "shell.execute_reply.started": "2021-09-24T18:43:12.630199Z"
    }
   },
   "outputs": [],
   "source": [
    "# load categories dataset\n",
    "cat_df = pd.read_csv(\"../input/disaster-response-messages/disaster_categories.csv\")\n",
    "cat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Merge datasets.\n",
    "- Merge the messages and categories datasets using the common id\n",
    "- Assign this combined dataset to `df`, which will be cleaned in the following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:12.954031Z",
     "iopub.status.busy": "2021-09-24T18:43:12.95304Z",
     "iopub.status.idle": "2021-09-24T18:43:12.991877Z",
     "shell.execute_reply": "2021-09-24T18:43:12.990899Z",
     "shell.execute_reply.started": "2021-09-24T18:43:12.953993Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge the datasets\n",
    "df = msg_df.merge(cat_df, left_on='id', right_on='id', how='inner')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split `categories` into separate category columns.\n",
    "- Split the values in the `categories` column on the `;` character so that each value becomes a separate column. You'll find [this method](https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.Series.str.split.html) very helpful! Make sure to set `expand=True`.\n",
    "- Use the first row of categories dataframe to create column names for the categories data.\n",
    "- Rename columns of `categories` with new column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:12.993584Z",
     "iopub.status.busy": "2021-09-24T18:43:12.993299Z",
     "iopub.status.idle": "2021-09-24T18:43:13.348498Z",
     "shell.execute_reply": "2021-09-24T18:43:13.347615Z",
     "shell.execute_reply.started": "2021-09-24T18:43:12.993557Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a dataframe of the 36 individual category columns\n",
    "categories = df[\"categories\"].str.split(';', expand=True)\n",
    "categories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:13.349921Z",
     "iopub.status.busy": "2021-09-24T18:43:13.349671Z",
     "iopub.status.idle": "2021-09-24T18:43:13.369902Z",
     "shell.execute_reply": "2021-09-24T18:43:13.369258Z",
     "shell.execute_reply.started": "2021-09-24T18:43:13.349891Z"
    }
   },
   "outputs": [],
   "source": [
    "# select the first row of the categories dataframe\n",
    "row = categories[0:1]\n",
    "\n",
    "# use this row to extract a list of new column names for categories.\n",
    "# one way is to apply a lambda function that takes everything \n",
    "# up to the second to last character of each string with slicing\n",
    "category_col = row.apply(lambda x: x.str[:-2]).values.tolist()\n",
    "print(category_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:13.371519Z",
     "iopub.status.busy": "2021-09-24T18:43:13.370888Z",
     "iopub.status.idle": "2021-09-24T18:43:13.411962Z",
     "shell.execute_reply": "2021-09-24T18:43:13.410996Z",
     "shell.execute_reply.started": "2021-09-24T18:43:13.371476Z"
    }
   },
   "outputs": [],
   "source": [
    "# rename the columns of `categories`\n",
    "categories.columns = category_col\n",
    "categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convert category values to just numbers 0 or 1.\n",
    "- Iterate through the category columns in df to keep only the last character of each string (the 1 or 0). For example, `related-0` becomes `0`, `related-1` becomes `1`. Convert the string to a numeric value.\n",
    "- You can perform [normal string actions on Pandas Series](https://pandas.pydata.org/pandas-docs/stable/text.html#indexing-with-str), like indexing, by including `.str` after the Series. You may need to first convert the Series to be of type string, which you can do with `astype(str)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:13.413541Z",
     "iopub.status.busy": "2021-09-24T18:43:13.413302Z",
     "iopub.status.idle": "2021-09-24T18:43:16.197857Z",
     "shell.execute_reply": "2021-09-24T18:43:16.197097Z",
     "shell.execute_reply.started": "2021-09-24T18:43:13.413515Z"
    }
   },
   "outputs": [],
   "source": [
    "for column in categories:\n",
    "    # set each value to be the last character of the string\n",
    "    categories[column] = categories[column].str[-1]\n",
    "    \n",
    "    # convert column from string to numeric\n",
    "    categories[column] = pd.to_numeric(categories[column])\n",
    "categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Replace `categories` column in `df` with new category columns.\n",
    "- Drop the categories column from the df dataframe since it is no longer needed.\n",
    "- Concatenate df and categories data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:16.199373Z",
     "iopub.status.busy": "2021-09-24T18:43:16.199126Z",
     "iopub.status.idle": "2021-09-24T18:43:16.219031Z",
     "shell.execute_reply": "2021-09-24T18:43:16.218177Z",
     "shell.execute_reply.started": "2021-09-24T18:43:16.199345Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop the original categories column from `df`\n",
    "\n",
    "df.drop(['categories'], axis=1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:16.222002Z",
     "iopub.status.busy": "2021-09-24T18:43:16.22178Z",
     "iopub.status.idle": "2021-09-24T18:43:16.256991Z",
     "shell.execute_reply": "2021-09-24T18:43:16.25615Z",
     "shell.execute_reply.started": "2021-09-24T18:43:16.221977Z"
    }
   },
   "outputs": [],
   "source": [
    "# concatenate the original dataframe with the new `categories` dataframe\n",
    "df = pd.concat([df, categories], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Remove duplicates.\n",
    "- Check how many duplicates are in this dataset.\n",
    "- Drop the duplicates.\n",
    "- Confirm duplicates were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:16.258504Z",
     "iopub.status.busy": "2021-09-24T18:43:16.258276Z",
     "iopub.status.idle": "2021-09-24T18:43:16.310251Z",
     "shell.execute_reply": "2021-09-24T18:43:16.309451Z",
     "shell.execute_reply.started": "2021-09-24T18:43:16.258479Z"
    }
   },
   "outputs": [],
   "source": [
    "# check number of duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:16.311583Z",
     "iopub.status.busy": "2021-09-24T18:43:16.311346Z",
     "iopub.status.idle": "2021-09-24T18:43:16.364641Z",
     "shell.execute_reply": "2021-09-24T18:43:16.36347Z",
     "shell.execute_reply.started": "2021-09-24T18:43:16.311556Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:16.366187Z",
     "iopub.status.busy": "2021-09-24T18:43:16.365948Z",
     "iopub.status.idle": "2021-09-24T18:43:16.412996Z",
     "shell.execute_reply": "2021-09-24T18:43:16.41205Z",
     "shell.execute_reply.started": "2021-09-24T18:43:16.366159Z"
    }
   },
   "outputs": [],
   "source": [
    "# check number of duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline Preparation For Disaster Messages Classification\n",
    "Follow the instructions below to help you create your ML pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:16.414971Z",
     "iopub.status.busy": "2021-09-24T18:43:16.414566Z",
     "iopub.status.idle": "2021-09-24T18:43:16.42531Z",
     "shell.execute_reply": "2021-09-24T18:43:16.423594Z",
     "shell.execute_reply.started": "2021-09-24T18:43:16.414928Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "\n",
    "X = df.message\n",
    "y = df.iloc[:,4:]\n",
    "category_names = y.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:16.426815Z",
     "iopub.status.busy": "2021-09-24T18:43:16.4266Z",
     "iopub.status.idle": "2021-09-24T18:43:16.440624Z",
     "shell.execute_reply": "2021-09-24T18:43:16.439697Z",
     "shell.execute_reply.started": "2021-09-24T18:43:16.42679Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # detect all URL present in the messages\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    # replace URL with \"urlplaceholder\"\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:16.442445Z",
     "iopub.status.busy": "2021-09-24T18:43:16.442034Z",
     "iopub.status.idle": "2021-09-24T18:43:16.453059Z",
     "shell.execute_reply": "2021-09-24T18:43:16.452164Z",
     "shell.execute_reply.started": "2021-09-24T18:43:16.442404Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('cvect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:43:16.45459Z",
     "iopub.status.busy": "2021-09-24T18:43:16.454162Z",
     "iopub.status.idle": "2021-09-24T18:51:18.378798Z",
     "shell.execute_reply": "2021-09-24T18:51:18.378233Z",
     "shell.execute_reply.started": "2021-09-24T18:43:16.45447Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:51:18.380296Z",
     "iopub.status.busy": "2021-09-24T18:51:18.379659Z",
     "iopub.status.idle": "2021-09-24T18:51:39.200703Z",
     "shell.execute_reply": "2021-09-24T18:51:39.199676Z",
     "shell.execute_reply.started": "2021-09-24T18:51:18.380267Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred[123].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:51:39.202164Z",
     "iopub.status.busy": "2021-09-24T18:51:39.201911Z",
     "iopub.status.idle": "2021-09-24T18:51:39.621065Z",
     "shell.execute_reply": "2021-09-24T18:51:39.620165Z",
     "shell.execute_reply.started": "2021-09-24T18:51:39.202137Z"
    }
   },
   "outputs": [],
   "source": [
    "# y_pred = pipeline.predict(X_test)\n",
    "\n",
    "for i in range(36):\n",
    "    print(\"=======================\",y_test.columns[i],\"======================\")\n",
    "    print(classification_report(y_test.iloc[:,i], y_pred[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:51:39.62253Z",
     "iopub.status.busy": "2021-09-24T18:51:39.622301Z",
     "iopub.status.idle": "2021-09-24T18:51:39.634379Z",
     "shell.execute_reply": "2021-09-24T18:51:39.633395Z",
     "shell.execute_reply.started": "2021-09-24T18:51:39.622504Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:51:39.635908Z",
     "iopub.status.busy": "2021-09-24T18:51:39.635668Z",
     "iopub.status.idle": "2021-09-24T18:51:39.646276Z",
     "shell.execute_reply": "2021-09-24T18:51:39.645376Z",
     "shell.execute_reply.started": "2021-09-24T18:51:39.635881Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'clf__estimator__n_estimators': [ 100, 150],\n",
    "    'clf__estimator__min_samples_split': [2, 4],\n",
    "    \n",
    "}+\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters, verbose=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T18:51:39.648061Z",
     "iopub.status.busy": "2021-09-24T18:51:39.647833Z",
     "iopub.status.idle": "2021-09-24T21:15:34.000481Z",
     "shell.execute_reply": "2021-09-24T21:15:33.998359Z",
     "shell.execute_reply.started": "2021-09-24T18:51:39.648035Z"
    }
   },
   "outputs": [],
   "source": [
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T21:15:34.005171Z",
     "iopub.status.busy": "2021-09-24T21:15:34.004783Z",
     "iopub.status.idle": "2021-09-24T21:15:34.022561Z",
     "shell.execute_reply": "2021-09-24T21:15:34.021321Z",
     "shell.execute_reply.started": "2021-09-24T21:15:34.005118Z"
    }
   },
   "outputs": [],
   "source": [
    "cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T21:15:34.024163Z",
     "iopub.status.busy": "2021-09-24T21:15:34.023902Z",
     "iopub.status.idle": "2021-09-24T21:15:34.046011Z",
     "shell.execute_reply": "2021-09-24T21:15:34.045118Z",
     "shell.execute_reply.started": "2021-09-24T21:15:34.024133Z"
    }
   },
   "outputs": [],
   "source": [
    "#finding the best paramesters based on grip search\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T21:15:34.047494Z",
     "iopub.status.busy": "2021-09-24T21:15:34.047201Z",
     "iopub.status.idle": "2021-09-24T21:15:34.071401Z",
     "shell.execute_reply": "2021-09-24T21:15:34.070428Z",
     "shell.execute_reply.started": "2021-09-24T21:15:34.047463Z"
    }
   },
   "outputs": [],
   "source": [
    "#building new model\n",
    "optimised_model = cv.best_estimator_\n",
    "print (cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T21:15:34.073378Z",
     "iopub.status.busy": "2021-09-24T21:15:34.07272Z",
     "iopub.status.idle": "2021-09-24T21:15:55.760971Z",
     "shell.execute_reply": "2021-09-24T21:15:55.760004Z",
     "shell.execute_reply.started": "2021-09-24T21:15:34.073339Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = optimised_model.predict(X_test)\n",
    "\n",
    "for i in range(36):\n",
    "    print(\"=============================\",y_test.columns[i], '=================================')\n",
    "    print(classification_report(y_test.iloc[:,i], y_pred[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T21:15:55.76266Z",
     "iopub.status.busy": "2021-09-24T21:15:55.7624Z",
     "iopub.status.idle": "2021-09-24T21:15:58.414443Z",
     "shell.execute_reply": "2021-09-24T21:15:58.413469Z",
     "shell.execute_reply.started": "2021-09-24T21:15:55.76263Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(optimised_model, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To access the full project. Click [here](https://github.com/sidharth178/Disaster-Response-Pipeline)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you find this notebook useful, don't forget to UPVOTE ⏫.\n",
    "## Follow me on [Github](https://github.com/sidharth178). I used to upload good data science projects."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1578051,
     "sourceId": 2596995,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30120,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
